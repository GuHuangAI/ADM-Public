model:
  class_name: ddm.ddm_const.LatentDiffusion
  image_size: [256, 256]
  ckpt_path:
  ignore_keys: []
  only_model: False
  sampling_timesteps: 10
  loss_type: l2
  start_dist: normal
  perceptual_weight: 0
  scale_factor: 0.165
  scale_by_std: True
  default_scale: True
  scale_by_softsign: False
  eps: !!float 1e-4
  sigma_max: 1
  sigma_min: 0.01
  ldm: True
  weighting_loss: False
  use_disloss: False
  use_l1: True
  use_augment: False
  loss_main:
    class_name: ddm.loss.MSE_Loss
  loss_vlb:
    class_name: ddm.loss.MAE_Loss
  loss_dis:
    class_name: ddm.loss.MSE_Loss
  first_stage:
    class_name: ddm.encoder_decoder.AutoencoderKL
    embed_dim: 3
    lossconfig:
      disc_start: 20001
      kl_weight: 0.000001
      disc_weight: 0.5
    ddconfig:
      double_z: True
      z_channels: 3
      resolution: [ 256, 256 ]
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult: [ 1,2,4 ]  # num_down = len(ch_mult)-1
      num_res_blocks: 2
      attn_resolutions: [ ]
      dropout: 0.0
    ckpt_path: '/data/huang/pretrain_weights/ldm-ae/model-kl-d4.ckpt'
#    ckpt_path: "/data/huang/celebahq/results_ae_kl_256x256_d4/model-10.pt"
  unet:
    class_name: unet.cond_unet_t2i.Unet
    fix_bb: False
    cond_net: ViT-B/16
    dim: 320
    channels: 3
    out_mul: 1
    dim_mults: [ 1, 2, 4, 4 ] # num_down = len(dim_mults)
    cond_in_dim: 3
    cond_dim: 128
    cond_dim_mults: [ ] # num_down = len(cond_dim_mults)
    window_sizes1: [ [ 16, 16 ], [ 16, 16 ], [ 8, 8 ], [ 8, 8 ] ]
    window_sizes2: [ [ 16, 16 ], [ 16, 16 ], [ 8, 8 ], [ 8, 8 ] ]
    fourier_scale: 16
    cond_pe: False
    num_pos_feats: 128
    cond_feature_size: [ 64, 64 ]

data:
  class_name: ddm.webdata.WebdatasetReader
  data_type: web
  image_size: [256, 256]
#  img_folder: '/data/huang/cifar-10-python/'
  dara_root: "/MyFiles/data/"
  augment_horizontal_flip: True
#  split: train
  batch_size: 48
  num_prepro_workers: 4

eval:
  begin_ckpt: 1
  end_ckpt: 35
  num_samples: 30000
  batch_size: 64
  use_ema: True
  workdir: "/data/huang/cifar-10-python/results_ddm_const"
#  gt_stats: "/nfs/data/diffusion_data/cifar/cifar10/results_etp_const_sde4_ncsnpp6/gt_feature.pth"
  gt_stats:

trainer:
  gradient_accumulate_every: 2
  lr: !!float 4e-5
  min_lr: !!float 4e-6
  train_num_steps: 2000000
  save_and_sample_every: 20000
  log_freq: 500
  results_folder: "/data/huang/celebahq/results_ddm_const_cond_unet"
#  results_folder: "/data/huang/cifar-10-python/results_etp_const_sde4_ncsnpp10"
  amp: False
  fp16: False
  resume_milestone: 0
  test_before: True
  ema_update_after_step: 20000
  ema_update_every: 8

sampler:
  batch_size: 32
  out_channels: 3
  sample_num: 30000
  use_ema: True
  test_in_train: False
  ckpt_path: "/data/huang/celebahq/results_ddm_const_cond_unet/model-21.pt"
  save_folder: "/data/huang/celebahq/results_ddm_const_cond_unet/samples_model-21-step10"
  target_path: "/data/huang/celebahq/celeba_hq_256"